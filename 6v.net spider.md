##### BeautifulSoup

```
输入：
from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen("https://read.qidian.com/chapter/iP0bnQUpGeWXfJNNZ-YUzw2/DyxENW5Z-vBOBDFlr9quQA2")
bsobj = BeautifulSoup(html.read(),'lxml')
print(bsobj.p)
输出：
<p>仙侠</p>
```

**过程公式化：**

1.将大标题提取保存为目录

```
用到的储备：正则表达式，BeautifulSoup库，文件操作
```

2.将文字逐条提取保存



3.用正则表达式提取里面所有的链接



4.获取有价值的数据，以csv格式存储或者存储到数据库



5.下载保存图片和视频（多用于图片和视频网站的批量处理）

**通用代码：**

```python
encoding: ' '    #避免乱码
import ****      #引入第三方库
#预设请求头（反爬应对措施之一）
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.108 Safari/537.36 2345Explorer/8.8.3.16721'}
#保存文件为TXT
title = "lalalala"
with open('C:\\you\\desktop\\title.txt',"a+") as f:
  f.write(title)
  f.close()
```

**成功案例**

1.获取6v电影主页所有电影名称，主要运用列表，正则表达式，文件流操作等。6v电影是我用过近两年的

###### 2.



```python
1.获取6v电影主页所有电影名称

encoding: 'UTF-8'
#正则表达式真他妈没意思^_^……
#全部内容已可保存到txt
import csv
import time
import sys
import requests
import os
import re

from bs4 import BeautifulSoup
path_txt  =  "G:/mycode/gets_today.txt"
path_csv = "G:/mycode/gets_td.csv"
#写入TXT  /函数
def save(output):
#output = '\t'.join(list_nname)将列表转化为字符串的操作

        try:
                with open(path_txt,"a+") as f:
                        f.write(output)
        
        except  UnicodeEncodeError:
                        print("解码异常")
        else:
                #print("写入文件成功")
                f.close()

#按顺序输出并存储列表元素为TXT格式   /函数
def txt_opt(list_nname):
        i=1
        for x in list_nname:
                #序号+名称
                print("%d: "%(i)+x,end=' ')
                output2 = ": " + x +"\n"
                save("%d"%i+output2)
                print("写入文件成功")
                print()
                i=i+1
        save("输出时间："+time.ctime())
        print("输出时间："+time.ctime())
#存储列表元素为csv格式
def csv_opt(list_nname):
        with open('gets_td.csv','a+',encoding='UTF-8',newline='')as csvfile:#如果不指定newline='',则一行只有一个元素
                w = csv.writer(csvfile)
                w.writerows(list_nname)
                w.writerow(time.ctime())
#链接网址
link_baidu = r"http://www.baidu.com"
link_qidian = r"http://www.qidian.com"
link_qidian_paihang = r"https://www.qidian.com/rank?chn=21"
link_blog = r"http://www.santostang.com"
link_db250 = r"http://movie.douban.com/top250"
link_6v = r"http://www.6vhao.tv"
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.108 Safari/537.36 2345Explorer/8.8.3.16721'}
r = requests.get(link_6v,headers = headers)
#防止乱码
print(r.encoding)
r.encoding = 'gb2312'
html = r.text
#正则表达式部分

list_HTTP = re.findall("https://.*?com",html)
list_nname1 = re.findall('target="_blank">.*?(.*?)</a></li>',html)
list_nname2 = re.findall('target=_blank>.*?(.*?)</a>&nbsp;',html)#括号里的重复部分表示只选择缺失部分
list_nname3 = re.findall('title=".*?(.*?)"/><span></span></a><p><a href',html)
list_nname4 = re.findall('target="_blank"><img src=".*?(.*?)" title="',html)#海报图片的网址
list_nname = list_nname1 + list_nname2 + list_nname3 

#list_nname.sort()  #重新排序@_@，排序之前在列表最前方能看到最近更新电影


#输出列表元素数量，即电影数量，这种情况下中间不能有逗号@_@
print("共获取%s个电影名称："% len(list_nname))
txt_opt(list_nname)  #输出并保存为TXT文件
csv_opt(list_nname)  #输出并保存为csv文件

```



知识储备：**

1.正则表达式

```python
G:\mycode\pppppppppy\RE_zhengze.py
  
list = "abcdefghijklmnopqrstuvwxyz"
re,match('abc',list)
re.search('fgh',list)
re.findall('[0-9]+',list)
```

2.BeautifulSoup库